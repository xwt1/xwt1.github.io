# PQ

[ANN 之 Product Quantization - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/140548922)

[ANN召回算法之IVFPQ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/378725270)

[https://ieeexplore.ieee.org/abstract/document/5432202](https://ieeexplore.ieee.org/abstract/document/5432202)

## 2.1 PQ（Product Quantization）以及查询举例说明

加一部分名词解释：

- codebook : 原始向量空间被划分为多个低维子空间。这是通过将向量的维度分成几部分，每部分作为一个独立的子空间，每个子空间都有一个对应的 codebook。codebook 是一种查找表，存储了该子空间内的向量的聚类中心（即质心）。
- codewords：指的是某个codebook中的某个聚类中心点。
- 比如我下面举的例子中，一共有两个codebooks（4维分成了两个向量子空间，相当于两个codebooks），每个codebooks都是有两个元素0和1，每个元素代表一个聚类中心codeword。

关于理论部分上面的博客已经说的比较清楚了，但为了加深和确认理解，举一个具体的例子来说明PQ以及查询的过程

1. 首先是PQ量化数据集的过程，假设有这样一个4维度数据集，一共8个向量：
    
    
    | 向量1 | 0 | 0 | 3 | 0 |
    | --- | --- | --- | --- | --- |
    | 向量2 | 2 | 0 | 3 | 3 |
    | 向量3 | 0 | 2 | 6 | 0 |
    | 向量4 | 2 | 2 | 6 | 3 |
    | 向量5 | 6 | 0 | 0 | 0 |
    | 向量6 | 8 | 0 | 0 | 1 |
    | 向量7 | 6 | 2 | 1 | 0 |
    | 向量8 | 8 | 2 | 1 | 1 |
    
    设采用K-means，并且$K=2$，$M$代表子空间的数量,并且将原四维空间划分为两个子空间，也就是说每个子空间大小为$D/M = 4/2 =2$，这里借用一下上面博客中喜欢用的字母。所以最终会被切割成2个二维空间，切割出来的两个子空间中的子向量会是这个样子：
    
    | 子空间1： |  |  |
    | --- | --- | --- |
    | 子向量1 | 0 | 0 |
    | 子向量2 | 2 | 0 |
    | 子向量3 | 0 | 2 |
    | 子向量4 | 2 | 2 |
    | 子向量5 | 6 | 0 |
    | 子向量6 | 8 | 0 |
    | 子向量7 | 6 | 2 |
    | 子向量8 | 8 | 2 |
    
    | 子空间2： |  |  |
    | --- | --- | --- |
    | 子向量1 | 3 | 0 |
    | 子向量2 | 3 | 3 |
    | 子向量3 | 6 | 0 |
    | 子向量4 | 6 | 3 |
    | 子向量5 | 0 | 0 |
    | 子向量6 | 0 | 1 |
    | 子向量7 | 1 | 0 |
    | 子向量8 | 1 | 1 |
    
    其中子空间1中的子向量1代表向量1的前两维，其他类推。
    
    这时候会发现我造的数据实际上在每个子空间中都是两个正方形，那么根据K-means的结果，它们的重心应该在对角线交点上，我们设子空间1的前四个子向量形成的聚簇编号是0，且其重心为$(1,1)$， 后四个向量编号为1，其重心为$(7,7)$，同样子空间2的前四个子向量形成的聚簇编号是0，其重心为$(1.5,1.5)$， 后四个向量编号为1，重心为$(0.5,0.5)$，那么我们就可以改写数据集了，首先改写子空间：
    
    | 子空间1： |  |
    | --- | --- |
    | 子向量1 | 0 |
    | 子向量2 | 0 |
    | 子向量3 | 0 |
    | 子向量4 | 0 |
    | 子向量5 | 1 |
    | 子向量6 | 1 |
    | 子向量7 | 1 |
    | 子向量8 | 1 |
    
    | 子空间2： |  |
    | --- | --- |
    | 子向量1 | 0 |
    | 子向量2 | 0 |
    | 子向量3 | 0 |
    | 子向量4 | 0 |
    | 子向量5 | 1 |
    | 子向量6 | 1 |
    | 子向量7 | 1 |
    | 子向量8 | 1 |
    
    之后把这些子向量拼凑起来组成原来的向量，原来的数据集就变成了：
    
    | 向量1 | 0 | 0 |
    | --- | --- | --- |
    | 向量2 | 0 | 0 |
    | 向量3 | 0 | 0 |
    | 向量4 | 0 | 0 |
    | 向量5 | 1 | 1 |
    | 向量6 | 1 | 1 |
    | 向量7 | 1 | 1 |
    | 向量8 | 1 | 1 |
    
    可以看到**“维度”**缩小了一半，而且可以用二进制编码的方式储存，在存储开销上确实会有非常大的提升。这样PQ量化的部分就结束了
    
2. 接下来是如何做搜索呢？最早的PQ的作者给出了两种方式：ADC和SDC：
    1. 先说SDC（symmetric 对称），对于上面PQ的结果，如果此时来了一个query：
        
        
        | query | 0 | 2 | 6 | 0 |
        | --- | --- | --- | --- | --- |
        
        这时候要首先对其做量化，量化过程一样，直接写结果：
        
        | query | 0 | 0 |
        | --- | --- | --- |
        
        此时根据SDC的计算方式，query与前四个向量的$dist$值都是0，而后四个：
        
        $$
        dist = \sqrt{(7-1)^2+(7-1)^2} + \sqrt{(1.5-0.5)^2+(1.5-0.5)^2}=7\sqrt{2}
        $$
        
        故根据SDC的计算办法，query点离前四个点同样近且并列第一近，距离后四个都是$7\sqrt{2}$且并列第二远。因此如果KNN的K是1的话，按照SDC的说法选前四个哪一个都是最近的那个，看起来没那么准确。
        
    2. 再说ADC（非对称的），也就是说query不需要量化，同样与SDC一样的一个query，直接计算：
        
        对于前四个data来说：
        
        $$
        dist = \sqrt{(0-1)^2+(2 - 1)^2}+\sqrt{(6-1.5)^2+(0-1.5)^2} = \sqrt{2}+\frac{3\sqrt{10}}{2}
        $$
        
        对于后四个data来说：
        
        $$
        dist = \sqrt{(0-7)^2+(2 - 7)^2}+\sqrt{(6-0.5)^2+(0-0.5)^2}=\sqrt{74}+\frac{\sqrt{122}}{2}
        $$
        
        明眼看上去肯定前四个点离query点近一点，看起来ADC和SDC在我这个例子上得到了相同的结果，但是实际上ADC的精度更高，可以感性定性理解：
        
        因为ADC只有数据集做了量化，会产生量化误差，而query本身是没有量化误差的，故最终误差只会由数据集带来。
        
        而SDC由于对query和数据集都做了量化，故会同时带来数据集和query的误差，相对ADC来说会多了query量化带来的误差，故精度会低。
        
        但是计算速度SDC可以提前打表速度会快于ADC（虽然最终搜索复杂度是一样的？）
        
3. 总结：
    - 约定：假设向量数据集是$P$，数据集大小是$N$，向量的维度是$D$，将向量的维度划分为$M$个子空间（codebook），每个子空间的维度大小是$D'=\frac{D}{M}$，每个子空间中使用$K-Means$聚类出$K$个聚簇。查询向量设为$Q$。假设最终要找$k$个最近邻。
    - 具体实施步骤：
        - 统一前置步骤：
            1. 首先将向量数据集$P$中的所有向量乘积量化（对每一个子空间做聚类），形成$N$个$D'$维度的向量组成的数据集。
        - **SDC（需量化）**
            1. 构建预处理距离矩阵：通过一个构建一个大小为$O(MK^2)$的距离矩阵，预先算出$P$每一个子空间中，两两聚簇之间的距离，从而在接下来的查询操作中使用。
            2. 搜索查询：
                1. 首先按照同样的方法对查询向量$Q$进行量化，具体来讲就是决定$Q$的每一个子空间归属于对应子空间中的哪一个聚簇。
                2. 查询，利用距离矩阵，对于$P$中每一个向量$V$，计算每一个子空间中$V$对应的**聚簇**与$Q$对应的**聚簇**之间的距离，将每一个子空间中计算出来的距离的和作为最终$V$与$Q$的距离。对于每一个$V$，按照距离大小排序，选出最小的$k$个作为查询结果。
        - **ADC（不需量化）**
            1. 不需量化，直接进行搜索查询：
                1. 由于没有做量化操作，首先应该计算查询向量$Q$每一个子空间对应的子向量与该子空间中每一个聚簇的距离并存储下来也作为一张查询表用于后续查询使用。
                2. 查询，利用上一步的查询表，对于$P$中每一个向量$V$，计算每一个子空间中$V$对应的**聚簇**与$Q$对应的**原始子向量**的距离（注意与SDC的红字区别），将每一个子空间中计算出来的距离的和作为最终$V$与$Q$的距离。对于每一个$V$，按照距离大小排序，选出最小的$k$个作为查询结果。
    - 时间复杂度
        - **SDC（需量化）**
            - 预处理距离矩阵构建时间复杂度：
                
                为了建立预处理距离矩阵，以在查询中查表，我们需要计算每对聚类中心之间的距离。每个子空间中有 $K$ 个聚类中心，因此需要计算的距离对数为 $\frac{K(K-1)}{2}$。因此：
                
                - **单个子空间**：计算距离的时间复杂度是 $O(K^2\frac{D}{M})$
                - **所有子空间**：因为有 M 个子空间，总时间复杂度为：
                    
                    $$
                    O(MK^2\frac{D}{M}) = O(K^2D)
                    $$
                    
            - **查询时间复杂度**：$O(KD+NM)$
                - **查询向量$Q$量化时间：** $O(KD)$ 是查询向量$Q$的量化时间。计量每一个子空间中的每一个聚簇，一共有$KM$个。对于每一个聚簇，都需要与查询向量计算距离，做距离计算的次数是$\frac{D}{M}$，所以最终复杂度是$O(KM\times \frac{D}{M})=O(KD)$。通过这个操作，将每个向量如前面的例子那样转成一个量化后的$M$维向量。
                - $O(NM)$ 是**查询操作时间**，利用预先计算好的距离矩阵，查表计算量化后的查询向量与每一个数据向量（$N$个数据向量）在每一个子空间（$M$个子空间）之间的距离并将每个子空间的结果加起来作为最终结果。
            - **优点**：通过预计算距离查表，可以在实际查询中更快计算对称距离。
        - **ADC（不需量化）**
            - **查询时间复杂度**：$O(KD+NM)$
                - 其中$O(KD)$不是用于量化的复杂度，这个地方需要实时计算查询向量与每个子空间中每个聚簇的距离，以用于之后的查询操作中。计算的方式与SDC中基本相同，复杂度是一样的，但是最终结果不同。**SDC这步最终生成的量化后的查询向量，ADC这步生成的是查询向量与每一个聚簇的距离组成的距离表。**
                - $O(NM)$是查询复杂度。利用上一步（$O(KD)$这步）生成的距离表，计算查询向量与每个数据集中的向量间的距离，计算查询向量与每一个数据向量（$N$个数据向量）在每一个子空间（$M$个子空间）之间的距离并将每个子空间的结果加起来作为最终距离结果。
            - **优点**：无需对查询向量进行量化，适合需要保留原始查询向量精度的应用。
    - 空间复杂度
        - **ADC**
            - **存储需求**：临时计算距离表所需的空间为 $O(MK)$，但可视为常数忽略。
        - **SDC**
            - **存储需求**：需要存储每个子空间中聚类中心间的预计算距离，空间复杂度为 $O(MK^2)$。
    - 查询精度与速度
        - **ADC**
            - **精度**：**更高的查询精度，因为查询向量不受量化误差的影响。**
            - **速度**：查询速度略低于SDC，因为需要实时计算距离。(这个地方不知道怎么理解，感觉复杂度是一样的)。
        - **SDC**
            - **精度**：**查询精度受限于量化误差。**
            - **速度**：查询速度较快，由于距离查表的存在，实时计算负担减轻。
    
## 2.2 PQ+IVF:
    
    ADC和SDC其实最终的复杂度是$O(NM)$，其中$N$是query的数量，$M$是子空间的数量，比起暴力对比$O(ND)$（其中$D$是原先的维度）来说要快一点，但是还是不实际而且效果也不好，故需要一种好的办法来做搜索，于是IVF这种倒排索引的方式就被引入了。
    
    ![Untitled](https://raw.githubusercontent.com/xwt1/xwt1.github.io/main/_misc/picture/2024-07-13-PQ(Product_Quantization)/PQ%2BIVF.png)
    
    - 如上图，IVFPQ的大致构建步骤如下：
        1. 首先使用K-means算法在训练数据上训练一个粗糙量化器（Coarse Quantizer），用于将每一个数据$y$分配到不同的倒排索引中。
        2. 求每个向量$y$与其聚类中心（K-means是重心）$q_{c}(y)$的残差，形成残差向量$r(y)$。
        3. 对每个形成的残差向量做乘积量化(PQ)，并将乘积量化的结果按照某种排序方式以倒排索引的方式放入到以$q_{c}(y)$为头节点的倒排索引数据结构中。
    - IVFPQ的搜索过程(按照上图为ADC方式)如下：
        1. 首先查找关于搜索点最近的粗聚类索引号$q_{c}(x)$。
        2. 计算$x$与$q_c(x)$的残差$r(x)$
        3. 计算残差$r(x)$与以$q_{c}(x)$为头形成的数据结构中每一个元素的距离，采用了倒排索引，可以快速定位到第一个离它最近的向量，从而找到离$r(x)$最近的$K$个元素。（没看懂inverted list具体是怎么做的，感觉这种数据结构可以是数组，链表，堆等等。找不到什么例子），但总之它能快速找到$K$个元素。
    
    ## 2.2 PQ+IVF:
    
    ADC和SDC其实最终的复杂度是$O(NM)$，其中$N$是query的数量，$M$是子空间的数量，比起暴力对比$O(ND)$（其中$D$是原先的维度）来说要快一点，但是还是不实际而且效果也不好，故需要一种好的办法来做搜索，于是IVF这种倒排索引的方式就被引入了。
    
    ![Untitled](https://raw.githubusercontent.com/xwt1/xwt1.github.io/main/_misc/picture/2024-07-13-PQ(Product_Quantization)/PQ%2BIVF.png)
    
    - 如上图，IVFPQ的大致构建步骤如下：
        1. 首先使用K-means算法在训练数据上训练一个粗糙量化器（Coarse Quantizer），用于将每一个数据$y$分配到不同的倒排索引中。
        2. 求每个向量$y$与其聚类中心（K-means是重心）$q_{c}(y)$的残差，形成残差向量$r(y)$。
        3. 对每个形成的残差向量做乘积量化(PQ)，并将乘积量化的结果按照某种排序方式以倒排索引的方式放入到以$q_{c}(y)$为头节点的倒排索引数据结构中。
    - IVFPQ的搜索过程(按照上图为ADC方式)如下：
        1. 首先查找关于搜索点最近的粗聚类索引号$q_{c}(x)$。
        2. 计算$x$与$q_c(x)$的残差$r(x)$
        3. 计算残差$r(x)$与以$q_{c}(x)$为头形成的数据结构中每一个元素的距离，采用了倒排索引，可以快速定位到第一个离它最近的向量，从而找到离$r(x)$最近的$K$个元素。（没看懂inverted list具体是怎么做的，感觉这种数据结构可以是数组，链表，堆等等。找不到什么例子），但总之它能快速找到$K$个元素。
    
    ### **构建复杂度**
    
    构建PQ-IVF索引涉及以下主要步骤：
    
    1. **训练量化器**：这包括训练一个粗量化器（用于IVF）和一个或多个细量化器（用于PQ）。粗量化器通常通过k-means算法在数据集上训练得到，其复杂度为O(NDK)，其中K是粗簇（coarse quantizer centroids）的数量。细量化器的训练复杂度同样依赖于使用的算法（通常也是k-means），但由于它是在每个子空间上独立进行的，其总复杂度为O(NDB)，其中B是子空间的数量（也即PQ中的量化器数量）。
    2. **索引构建**：在训练了量化器之后，需要为每个向量计算它的粗簇ID（通过粗量化器）和在每个子空间的量化码（通过细量化器）。这一步的复杂度为O(ND(K+B))，因为每个向量都需要与K个粗簇中心和B个子空间的量化中心进行比较。
    
    综上，构建复杂度的总体估计为O(NDK + NDB + ND(K+B))。注意，这是一个简化的估计，实际的复杂度还可能受到其他因素的影响，如优化算法的选择和并行处理的使用。GPT生成的，构建复杂度与采用的聚类办法的复杂度相同。
    
    ### **搜索复杂度**
    
    搜索时，分析最暴力的情况下的复杂度：
    
    1. 找倒排索引的头，假设有$K$个粗粒度聚类，原向量维度为$D$，则这部分复杂度为$O(KD)$
    2. 重排：直接计算query与上一步找到的倒排索引中每一个元素的距离，假设每一个向量在每一个粗粒度簇中平均分配，那么每一个簇中有$\frac{N}{K}$个量化过的向量，计算一次距离为$D$（ADC计算距离必然为$D$），故这部分的时间复杂度为$O(\frac{N}{K}D)$，这时如果想求前$T$个最大的，就需要对这些距离直接进行排序（或者写个小根堆），复杂度为$O(\frac{N}{K}log(\frac{N}{K}))$，故最暴力的重排的复杂度为$O(\frac{N}{K}D + \frac{N}{K}log(\frac{N}{K}))$。
    
    故搜索的总复杂度为$O(KD+\frac{N}{K}D + \frac{N}{K}log(\frac{N}{K}))$。