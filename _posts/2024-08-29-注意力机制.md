# 注意力机制

# 1. 原生注意力机制

## 1.1 机制简介

[https://www.cnblogs.com/jins-note/p/13056604.html](https://www.cnblogs.com/jins-note/p/13056604.html) //参考博客

这个地方只是抽象的介绍一下注意力机制的过程，实际数学上（如矩阵的维度等信息）可能是错的，见谅，有空再详细补。

为了更好的理解注意力机制，我们全程通过encoder-decoder经典模型来介绍。

什么是注意力机制呢？说白了，就是模拟人类对于特定事物的注意能力，比如你在看一张照片的时候，不可能照片中每一个像素都会引起你的注意，一定有某些特定的东西能够吸引你的注意力。同样的，在做文本任务或者其他深度学习任务时，也需要类似的机制，保证网络不被大量无用的信息吞没。

![image.png](https://raw.githubusercontent.com/xwt1/xwt1.github.io/main/_misc/picture/2024-08-29-attention_mechanism/image.png)

以最古老的encoder-decoder模型举例（后文也结合该模型理解注意力机制），假设encoder和decoder都采用的是RNN模型，输入单词的数量为$in$，每个单词用特定形式的向量表示（如one-hot）。encoder最终的输出设为$h_{in}$（**在上图中，是语义编码c**），在原始的encoder-decoder架构中，$h_{in}$被直接作为decoder的输入被输入进网络中。很明显，这样直接将其输入进网络中，相当于对于每一个被输入的单词都有相同的重视程度，没有侧重点，重要信息容易被大量无用信息遮盖。

在这样的背景下，注意力机制为了解决关键信息被大量无用信息遮盖的问题而诞生了（并且记住，注意力机制不是只能运用在encoder-decoder模型中，在其余地方也可以使用，如图像识别）。

## 1.2 粗略数学过程理解

![image.png](https://raw.githubusercontent.com/xwt1/xwt1.github.io/main/_misc/picture/2024-08-29-attention_mechanism/image%201.png)

![image.png](https://raw.githubusercontent.com/xwt1/xwt1.github.io/main/_misc/picture/2024-08-29-attention_mechanism/image%202.png)

上面这两张图是比较笼统的方式介绍了注意力机制的运行机制，对于第一张图，我们与encoder-decoder类比来理解其中每一个量代表什么：

![image.png](https://raw.githubusercontent.com/xwt1/xwt1.github.io/main/_misc/picture/2024-08-29-attention_mechanism/image%203.png)

![image.png](https://raw.githubusercontent.com/xwt1/xwt1.github.io/main/_misc/picture/2024-08-29-attention_mechanism/image%204.png)

- QUERY : 一组向量构成的矩阵，其在encoder-decoder架构中，是所有decoder中的所有隐藏层向量构成的，在上图中，就是$[y_1,y_2,…y_{n-1}]$构成的矩阵，而上图中的Query，其实指的是某一个时刻，比如$i$时刻，decoder的隐藏层向量$y_i$。所以其实可以写成：$QUERY=[y_1,y_2,…y_{n-1}]$。
- KEY：一组向量组成的矩阵，其在encoder-decoder架构中，是所有encoder中的所有隐藏层向量构成的，在上图中，就是$[x_1,x_2,...,x_m]$构成的矩阵，或者根据上图写成这样：$KEY = [x_1,x_2,...,x_m]=[Key_1,Key_2,...,Key_m]$。
- VALUE : 一组向量组成的矩阵，在**一般情况下**，与KEY一样，是所有encoder中的所有隐藏层向量构成的。仔细观察上图可知，其是softmax生成权重$[a_1,a_2,...,a_n]$后用来被挑选的encoder向量。

介绍完基本的绍完名词后，我们大概结合一下encoder-decoder的架构说一下注意力机制的计算流程，抽象流程如第二张图所示：

1. 首先对于任意一个Query，也就是decoder要输出的隐藏层向量，计算出其与每一个encoder向量的相似度。也就是图中的$F(Q,K)$。相似度函数可以使用余弦度量等方式。
2. 其次利用类似Softmax函数，将相似度向量归一化，得到真正的权重向量$A=[a_1,a_2,...,a_n]$，这一步在很多博客和文章中是下面这个式子：
    
    $$
    A=softmax(\frac{Query \times Key}{\sqrt{d_k}})
    $$
    
    缩放因子（scaling factor）$\sqrt{d_k}$ 是用来缩放注意力权重分布的一个系数（据说是为了稳定梯度）
    
3. 最终计算注意力分数：
    
    $$
    Attention\_Value = A \times VALUE
    $$
    
    得到的注意力分数，在上图中，就是指在decoder中，每一个时刻$i$与隐藏层向量$y_i$一同送入网络中的$C_i$（如上第四张图中的$C_1,C_2,C_3$，也可以理解成第三张图中，对于decoder中的每一个时刻$i$，计算出的其注意力分数$C_i$，与$y_i$输入到每一个$H_i$块中）。
    

将注意力机制应用到网络后，在后续的训练过程中，可以与网络一起进行正常的前推+反向传播更新其参数。