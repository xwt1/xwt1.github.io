# PQ

[ANN 之 Product Quantization - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/140548922)

[ANN召回算法之IVFPQ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/378725270)

[https://ieeexplore.ieee.org/abstract/document/5432202](https://ieeexplore.ieee.org/abstract/document/5432202)

## 2.1 PQ（Product Quantization）以及查询举例说明

加一部分名词解释：

- codebooks : 指的是子空间的数量，每个codebook都代表一个子空间中聚类中心点的集合
- codewords：指的是某个codebook中的某个聚类中心点。
- 比如我下面举的例子中，一共有两个codebooks（4维分成了两个向量子空间，相当于两个codebooks），每个codebooks都是有两个元素0和1，每个元素代表一个聚类中心codeword。

关于理论部分上面的博客已经说的比较清楚了，但为了加深和确认理解，举一个具体的例子来说明PQ以及查询的过程

1. 首先是PQ量化数据集的过程，假设有这样一个4维度数据集，一共8个向量：
    
    
    | 向量1 | 0 | 0 | 3 | 0 |
    | --- | --- | --- | --- | --- |
    | 向量2 | 2 | 0 | 3 | 3 |
    | 向量3 | 0 | 2 | 6 | 0 |
    | 向量4 | 2 | 2 | 6 | 3 |
    | 向量5 | 6 | 0 | 0 | 0 |
    | 向量6 | 8 | 0 | 0 | 1 |
    | 向量7 | 6 | 2 | 1 | 0 |
    | 向量8 | 8 | 2 | 1 | 1 |
    
    设采用K-means，并且$K=2$，并且将原四维空间划分为两个子空间，也就是说每个子空间大小为$D/M = 4/2 =2$，这里借用一下上面博客中喜欢用的字母。所以最终会被切割成2个二维空间，切割出来的两个子空间中的子向量会是这个样子：
    
    | 子空间1： |  |  |
    | --- | --- | --- |
    | 子向量1 | 0 | 0 |
    | 子向量2 | 2 | 0 |
    | 子向量3 | 0 | 2 |
    | 子向量4 | 2 | 2 |
    | 子向量5 | 6 | 0 |
    | 子向量6 | 8 | 0 |
    | 子向量7 | 6 | 2 |
    | 子向量8 | 8 | 2 |
    
    | 子空间2： |  |  |
    | --- | --- | --- |
    | 子向量1 | 3 | 0 |
    | 子向量2 | 3 | 3 |
    | 子向量3 | 6 | 0 |
    | 子向量4 | 6 | 3 |
    | 子向量5 | 0 | 0 |
    | 子向量6 | 0 | 1 |
    | 子向量7 | 1 | 0 |
    | 子向量8 | 1 | 1 |
    
    其中子空间1中的子向量1代表向量1的前两维，其他类推。
    
    这时候会发现我造的数据实际上在每个子空间中都是两个正方形，那么根据K-means的结果，它们的重心应该在对角线交点上，我们设子空间1的前四个子向量形成的聚簇编号是0，且其重心为$(1,1)$， 后四个向量编号为1，其重心为$(7,7)$，同样子空间2的前四个子向量形成的聚簇编号是0，其重心为$(1.5,1.5)$， 后四个向量编号为1，重心为$(0.5,0.5)$，那么我们就可以改写数据集了，首先改写子空间：
    
    | 子空间1： |  |
    | --- | --- |
    | 子向量1 | 0 |
    | 子向量2 | 0 |
    | 子向量3 | 0 |
    | 子向量4 | 0 |
    | 子向量5 | 1 |
    | 子向量6 | 1 |
    | 子向量7 | 1 |
    | 子向量8 | 1 |
    
    | 子空间2： |  |
    | --- | --- |
    | 子向量1 | 0 |
    | 子向量2 | 0 |
    | 子向量3 | 0 |
    | 子向量4 | 0 |
    | 子向量5 | 1 |
    | 子向量6 | 1 |
    | 子向量7 | 1 |
    | 子向量8 | 1 |
    
    之后把这些子向量拼凑起来组成原来的向量，原来的数据集就变成了：
    
    | 向量1 | 0 | 0 |
    | --- | --- | --- |
    | 向量2 | 0 | 0 |
    | 向量3 | 0 | 0 |
    | 向量4 | 0 | 0 |
    | 向量5 | 1 | 1 |
    | 向量6 | 1 | 1 |
    | 向量7 | 1 | 1 |
    | 向量8 | 1 | 1 |
    
    可以看到**“维度”**缩小了一半，而且可以用二进制编码的方式储存，在存储开销上确实会有非常大的提升。这样PQ量化的部分就结束了
    
2. 接下来是如何做搜索呢？最早的PQ的作者给出了两种方式：ADC和SDC：
    1. 先说SDC（symmetric 对称），对于上面PQ的结果，如果此时来了一个query：
        
        
        | query | 0 | 2 | 6 | 0 |
        | --- | --- | --- | --- | --- |
        
        这时候要首先对其做量化，量化过程一样，直接写结果：
        
        | query | 0 | 0 |
        | --- | --- | --- |
        
        此时根据SDC的计算方式，query与前四个向量的$dist$值都是0，而后四个：
        
        $$
        dist = \sqrt{(7-1)^2+(7-1)^2} + \sqrt{(1.5-0.5)^2+(1.5-0.5)^2}=7\sqrt{2}
        $$
        
        故根据SDC的计算办法，query点离前四个点同样近且并列第一近，距离后四个都是$7\sqrt{2}$且并列第二远。因此如果KNN的K是1的话，按照SDC的说法选前四个哪一个都是最近的那个，看起来没那么准确。
        
    2. 再说ADC（非对称的），也就是说query不需要量化，同样与SDC一样的一个query，直接计算：
        
        对于前四个data来说：
        
        $$
        dist = \sqrt{(0-1)^2+(2 - 1)^2}+\sqrt{(6-1.5)^2+(0-1.5)^2} = \sqrt{2}+\frac{3\sqrt{10}}{2}
        $$
        
        对于后四个data来说：
        
        $$
        dist = \sqrt{(0-7)^2+(2 - 7)^2}+\sqrt{(6-0.5)^2+(0-0.5)^2}=\sqrt{74}+\frac{\sqrt{122}}{2}
        $$
        
        明眼看上去肯定前四个点离query点近一点，看起来ADC和SDC在我这个例子上得到了相同的结果，但是实际上ADC的精度更高，可以感性定性理解：
        
        因为ADC只有数据集做了量化，会产生量化误差，而query本身是没有量化误差的，故最终误差只会由数据集带来。
        
        而SDC由于对query和数据集都做了量化，故会同时带来数据集和query的误差，相对ADC来说会多了query量化带来的误差，故精度会低。
        
        但是计算速度SDC可以提前打表速度会快于ADC
        
    
## 2.2 PQ+IVF:
    
ADC和SDC其实最终的复杂度是$O(NM)$，其中$N$是query的数量，$M$是子空间的数量，比起暴力对比$O(ND)$（其中$D$是原先的维度）来说要快一点，但是还是不实际而且效果也不好，故需要一种好的办法来做搜索，于是IVF这种倒排索引的方式就被引入了。

![Untitled](https://github.com/xwt1/xwt1.github.io/tree/main/_misc/picture/2024-07-13-PQ(Product_Quantization)/PQ+IVF.png)

- 如上图，IVFPQ的大致构建步骤如下：
    1. 首先使用K-means算法在训练数据上训练一个粗糙量化器（Coarse Quantizer），用于将每一个数据$y$分配到不同的倒排索引中。
    2. 求每个向量$y$与其聚类中心（K-means是重心）$q_{c}(y)$的残差，形成残差向量$r(y)$。
    3. 对每个形成的残差向量做乘积量化(PQ)，并将乘积量化的结果按照某种排序方式以倒排索引的方式放入到以$q_{c}(y)$为头节点的倒排索引数据结构中。
- IVFPQ的搜索过程(按照上图为ADC方式)如下：
    1. 首先查找关于搜索点最近的粗聚类索引号$q_{c}(x)$。
    2. 计算$x$与$q_c(x)$的残差$r(x)$
    3. 计算残差$r(x)$与以$q_{c}(x)$为头形成的数据结构中每一个元素的距离，采用了倒排索引，可以快速定位到第一个离它最近的向量，从而找到离$r(x)$最近的$K$个元素。（没看懂inverted list具体是怎么做的，感觉这种数据结构可以是数组，链表，堆等等。找不到什么例子），但总之它能快速找到$K$个元素。

### 2.2.1 **构建复杂度**

构建PQ-IVF索引涉及以下主要步骤：

1. **训练量化器**：这包括训练一个粗量化器（用于IVF）和一个或多个细量化器（用于PQ）。粗量化器通常通过k-means算法在数据集上训练得到，其复杂度为O(NDK)，其中K是粗簇（coarse quantizer centroids）的数量。细量化器的训练复杂度同样依赖于使用的算法（通常也是k-means），但由于它是在每个子空间上独立进行的，其总复杂度为O(NDB)，其中B是子空间的数量（也即PQ中的量化器数量）。
2. **索引构建**：在训练了量化器之后，需要为每个向量计算它的粗簇ID（通过粗量化器）和在每个子空间的量化码（通过细量化器）。这一步的复杂度为O(ND(K+B))，因为每个向量都需要与K个粗簇中心和B个子空间的量化中心进行比较。

综上，构建复杂度的总体估计为O(NDK + NDB + ND(K+B))。注意，这是一个简化的估计，实际的复杂度还可能受到其他因素的影响，如优化算法的选择和并行处理的使用。GPT生成的，构建复杂度与采用的聚类办法的复杂度相同。

### 2.2.2 **搜索复杂度**

搜索时，分析最暴力的情况下的复杂度：

1. 找倒排索引的头，假设有$K$个粗粒度聚类，原向量维度为$D$，则这部分复杂度为$O(KD)$
2. 重排：直接计算query与上一步找到的倒排索引中每一个元素的距离，假设每一个向量在每一个粗粒度簇中平均分配，那么每一个簇中有$\frac{N}{K}$个量化过的向量，计算一次距离为$D$（ADC计算距离必然为$D$），故这部分的时间复杂度为$O(\frac{N}{K}D)$，这时如果想求前$T$个最大的，就需要对这些距离直接进行排序（或者写个小根堆），复杂度为$O(\frac{N}{K}log(\frac{N}{K}))$，故最暴力的重排的复杂度为$O(\frac{N}{K}D + \frac{N}{K}log(\frac{N}{K}))$。

故搜索的总复杂度为$O(KD+\frac{N}{K}D + \frac{N}{K}log(\frac{N}{K}))$。